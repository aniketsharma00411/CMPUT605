# -*- coding: utf-8 -*-
"""selective_reinitialization_unlearning_MNIST_3_fraction_retraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_D2OY0un5ZOih8kI0a2vTYGisrA6nzom
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

import numpy as np

import json

num_runs = 10

"""# Data"""

# Transform image to tensor and normalize features from [0,255] to [0,1]
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.5,), (0.5,)),
                                ])

# Using MNIST
traindata = datasets.MNIST('data', download=True,
                           train=True, transform=transform)
testdata = datasets.MNIST('data', download=True,
                          train=False, transform=transform)

# Loaders that give 64 example batches
all_data_train_loader = torch.utils.data.DataLoader(
    traindata, batch_size=64, shuffle=True)
all_data_test_loader = torch.utils.data.DataLoader(
    testdata, batch_size=64, shuffle=True)

num_train_samples = len(all_data_train_loader.dataset)
num_test_samples = len(all_data_test_loader.dataset)

# Test dataloader with 3's only
threes_index = []
nonthrees_index = []
for i in range(0, len(testdata)):
    if testdata[i][1] == 3:
        threes_index.append(i)
    else:
        nonthrees_index.append(i)
three_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64,
                                                sampler=torch.utils.data.SubsetRandomSampler(threes_index))
nonthree_test_loader = torch.utils.data.DataLoader(testdata, batch_size=64,
                                                   sampler=torch.utils.data.SubsetRandomSampler(nonthrees_index))
num_three_test_samples = len(threes_index)
num_nonthree_test_samples = len(nonthrees_index)

# Train dataloaders with just 3s
nonthrees_index = []
threes_index = []
count = 0
for i in range(0, len(traindata)):
    if traindata[i][1] != 3:
        nonthrees_index.append(i)
        # threes_index.append(i)
#   if traindata[i][1] == 3 and count < 100:
    if traindata[i][1] == 3:
        # count += 1
        threes_index.append(i)
nonthree_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64,
                                                    sampler=torch.utils.data.SubsetRandomSampler(nonthrees_index))
three_train_loader = torch.utils.data.DataLoader(traindata, batch_size=64,
                                                 sampler=torch.utils.data.SubsetRandomSampler(threes_index))
num_three_train_samples = len(threes_index)
num_nonthree_train_samples = len(nonthrees_index)

"""# Model Training"""


class SimpleFNN(nn.Module):
    def __init__(self):
        super(SimpleFNN, self).__init__()
        self.layers = nn.ModuleList()
        self.fc1 = nn.Linear(28*28, 128)
        self.layers.append(self.fc1)
        self.fc2 = nn.Linear(128, 64)
        self.layers.append(self.fc2)
        self.fc3 = nn.Linear(64, 10)
        self.layers.append(self.fc3)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def test(model, loader, dname="Test set", printable=True):
    model.eval()
    test_loss = 0
    total = 0
    correct = 0
    with torch.no_grad():
        for data, target in loader:
            data = data.view(-1, 28*28)
            output = model(data)
            total += target.size()[0]
            test_loss += criterion(output, target).item()
            pred = output.data.max(1, keepdim=True)[1]
            correct += pred.eq(target.data.view_as(pred)).sum()

    test_loss /= total
    if printable:
        print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(
            dname, test_loss, correct, total,
            100. * correct / total
        ))
    else:
        return float(1. * correct / total)


def get_node_utilities(model, retain_data_loader, forget_data_loader):
    activations = {}

    def get_activation(name):
        def hook(model, input, output):
            activations[name] = output.detach()
        return hook

    model.layers[0].register_forward_hook(get_activation('layer_0'))
    model.layers[1].register_forward_hook(get_activation('layer_1'))
    model.layers[2].register_forward_hook(get_activation('layer_2'))

    forget_activations = {}
    for batch_idx, (data, target) in enumerate(forget_data_loader):
        data = data.view(-1, 28*28)
        output = model(data)

        for layer_name, layer_activations in activations.items():
            if layer_name in forget_activations:
                forget_activations[layer_name] = torch.cat(
                    (forget_activations[layer_name], activations[layer_name]))
            else:
                forget_activations[layer_name] = torch.clone(
                    activations[layer_name])

    retain_activations = {}
    for batch_idx, (data, target) in enumerate(retain_data_loader):
        data = data.view(-1, 28*28)
        output = model(data)

        for layer_name, layer_activations in activations.items():
            if layer_name in retain_activations:
                retain_activations[layer_name] = torch.cat(
                    (retain_activations[layer_name], activations[layer_name]))
            else:
                retain_activations[layer_name] = torch.clone(
                    activations[layer_name])

    utilities = []
    for (forget_layer_name, forget_activations), (retain_layer_name, retain_activations) in zip(forget_activations.items(), retain_activations.items()):
        assert forget_layer_name == retain_layer_name
        utility = forget_activations.mean(
            dim=0) - retain_activations.mean(dim=0)
        utilities.append(utility)

    return utilities


def reinitialize_nodes(model, utilities, alpha=0.1):
    params = {}
    for name, param in model.named_parameters():
        params[name] = param

    num_layers = len(model.layers)
    for i, layer in enumerate(model.layers):
        if i == num_layers-1:
            break

        layer_utilities = utilities[i]
        num_neurons = layer.out_features

        num_to_reinit = int(alpha * num_neurons)

        _, indices = torch.topk(
            layer_utilities, k=num_to_reinit, largest=True)

        param = params[f'layers.{i+1}.weight']

        for idx in indices:
            if layer_utilities[idx] <= 0:
                continue
            # param.data[:, idx] = torch.randn_like(param.data[:, idx])
            param.data[:, idx] = torch.zeros_like(param.data[:, idx])


alpha_results = {}
for alpha in [0.01, 0.05, 0.1, 0.15, 0.25, 0.5, 0.75, 1]:
    print("Alpha: ", alpha)
    results = []
    for run in range(num_runs):
        print("Run: ", run+1)
        run_results = {}
        model = SimpleFNN()
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

        epochs = 1
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(all_data_train_loader):
                data = data.view(-1, 28*28)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()

                optimizer.step()

                if batch_idx % 100 == 0:
                    print(
                        f"Train Epoch: {epoch} [{batch_idx * len(data)}/{num_train_samples} ({100. * batch_idx / len(all_data_train_loader):.0f}%)]\tLoss: {loss.item():.6f}")

        # test(model, all_data_train_loader, dname="Training data")
        # test(model, all_data_test_loader, dname="Test data")
        # test(model, three_train_loader, dname="Training data with only 3s")
        # test(model, nonthree_train_loader, dname="Training data except 3s")
        # test(model, three_test_loader, dname="Test data with only 3s")
        # test(model, nonthree_test_loader, dname="Test data except 3s")

        run_results['After Training'] = {}
        run_results['After Training']['Test Set Accuracy'] = test(
            model, all_data_test_loader, dname="Test data", printable=False)
        run_results['After Training']['Forget Set Accuracy'] = test(
            model, three_train_loader, dname="Training data with only 3s", printable=False)
        run_results['After Training']['Retain Set Accuracy'] = test(
            model, nonthree_train_loader, dname="Training data except 3s", printable=False)
        run_results['After Training']['Test (Forget) Set Accuracy'] = test(
            model, three_test_loader, dname="Test data with only 3s", printable=False)
        run_results['After Training']['Test (Retain) Set Accuracy'] = test(
            model, nonthree_test_loader, dname="Test data except 3s", printable=False)

        print("Training Complete")

        """# Calculating Node Utilities"""

        utilities = get_node_utilities(
            model, nonthree_train_loader, three_train_loader)

        utilities

        """# Selective Reinitialization"""

        reinitialize_nodes(model, utilities, alpha=alpha)

        # test(model, all_data_train_loader, dname="Training data")
        # test(model, all_data_test_loader, dname="Test data")
        # test(model, three_train_loader, dname="Training data with only 3s")
        # test(model, nonthree_train_loader, dname="Training data except 3s")
        # test(model, three_test_loader, dname="Test data with only 3s")
        # test(model, nonthree_test_loader, dname="Test data except 3s")

        run_results['After Selective Reinitialization'] = {}
        run_results['After Selective Reinitialization']['Test Set Accuracy'] = test(
            model, all_data_test_loader, dname="Test data", printable=False)
        run_results['After Selective Reinitialization']['Forget Set Accuracy'] = test(
            model, three_train_loader, dname="Training data with only 3s", printable=False)
        run_results['After Selective Reinitialization']['Retain Set Accuracy'] = test(
            model, nonthree_train_loader, dname="Training data except 3s", printable=False)
        run_results['After Selective Reinitialization']['Test (Forget) Set Accuracy'] = test(
            model, three_test_loader, dname="Test data with only 3s", printable=False)
        run_results['After Selective Reinitialization']['Test (Retain) Set Accuracy'] = test(
            model, nonthree_test_loader, dname="Test data except 3s", printable=False)

        print("Unlearning Complete")

        """# Retraining on retain data"""

        np.random.shuffle(nonthrees_index)
        small_nonthrees_index = nonthrees_index[:int(0.01*len(traindata))]
        retain_small_data_loader = torch.utils.data.DataLoader(traindata, batch_size=64,
                                                               sampler=torch.utils.data.SubsetRandomSampler(small_nonthrees_index))
        num_retain_small_samples = len(small_nonthrees_index)

        torch.save(model.state_dict(), 'model.pth')
        ft_model = SimpleFNN()
        ft_model.load_state_dict(torch.load('model.pth'))
        optimizer = optim.SGD(ft_model.parameters(), lr=0.01, momentum=0.9)

        epochs = 1
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(retain_small_data_loader):
                data = data.view(-1, 28*28)
                optimizer.zero_grad()
                output = ft_model(data)
                loss = criterion(output, target)
                loss.backward()

                optimizer.step()

                if batch_idx % 100 == 0:
                    print(
                        f"Train Epoch: {epoch} [{batch_idx * len(data)}/{num_retain_small_samples} ({100. * batch_idx / len(retain_small_data_loader):.0f}%)]\tLoss: {loss.item():.6f}")

        # test(ft_model, all_data_train_loader, dname="Training data")
        # test(ft_model, all_data_test_loader, dname="Test data")
        # test(ft_model, three_train_loader, dname="Training data with only 3s")
        # test(ft_model, nonthree_train_loader, dname="Training data except 3s")
        # test(ft_model, three_test_loader, dname="Test data with only 3s")
        # test(ft_model, nonthree_test_loader, dname="Test data except 3s")

        run_results['After Fine Tuning'] = {}
        run_results['After Fine Tuning']['Test Set Accuracy'] = test(
            ft_model, all_data_test_loader, dname="Test data", printable=False)
        run_results['After Fine Tuning']['Forget Set Accuracy'] = test(
            ft_model, three_train_loader, dname="Training data with only 3s", printable=False)
        run_results['After Fine Tuning']['Retain Set Accuracy'] = test(
            ft_model, nonthree_train_loader, dname="Training data except 3s", printable=False)
        run_results['After Fine Tuning']['Test (Forget) Set Accuracy'] = test(
            ft_model, three_test_loader, dname="Test data with only 3s", printable=False)
        run_results['After Fine Tuning']['Test (Retain) Set Accuracy'] = test(
            ft_model, nonthree_test_loader, dname="Test data except 3s", printable=False)

        print("Fine Tuning Complete")

        results.append(run_results)

    alpha_results[alpha] = results

with open("/home/aniket7/unlearning_exps/selective_reinitialization_unlearning_mnist_3_fraction_retraining_alpha_ht/selective_reinitialization_unlearning_mnist_3_fraction_retraining_alpha_ht.json", "w") as f:
    json.dump(alpha_results, f, indent=4)
